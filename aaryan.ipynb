{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('gemini_key')\n",
    "genai.configure(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Write a story about a magic backpack.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample.mov\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m video_file \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241m.\u001b[39mupload_file(path\u001b[38;5;241m=\u001b[39mfile_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(video_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'genai' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = \"example.mov\"\n",
    "video_file = genai.upload_file(path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Check whether the file is ready to be used.\n",
    "while video_file.state.name == \"PROCESSING\":\n",
    "    print('.', end='')\n",
    "    time.sleep(10)\n",
    "    video_file = genai.get_file(video_file.name)\n",
    "\n",
    "if video_file.state.name == \"FAILED\":\n",
    "  raise ValueError(video_file.state.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TODO: Add jamba api key and prompt jamba\n",
    "\n",
    "task = \"Developing a native application to stop procrastination using screen recording and multi-modal models\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"The user is doing the following task: {task}. If they are doing the task and not another task, say well done and give any helpful advice if they are stuck. If they\n",
    "are not doing the task, gently prompt them to go back to the relevant application window and do the task.\"\"\"\n",
    "\n",
    "def generate_prompt(task):\n",
    "    prompt = f\"\"\"This is a user doing a task. They should be doing the following task: {task}. \n",
    "If they're doing the task, say well done and if you have any helpful advice if they're stuck help them out. If they're not doing the task, gently prompt them to do the task.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "response = model.generate_content([video_file, generate_prompt(task)],\n",
    "                                  request_options={\"timeout\": 600})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like the user is exploring Google's Gemini AI API for video processing. This is a great starting point for a project!  To help with your procrastination-stopping app, you could use the following:\n",
      "\n",
      "* **Screen Recording:** Integrate a screen recording feature into your app. This could be done using libraries like `PyAutoGUI` for Python. \n",
      "* **Multi-modal Models:**  Gemini AI's video processing API could help analyze the recorded content.  You could use it to:\n",
      "    * **Identify procrastination triggers:**  Analyze the content of the recordings to see what websites, activities, or moments lead to procrastination.\n",
      "    * **Generate motivational content:** Based on the analysis, create personalized messages or reminders to help the user stay focused.\n",
      "* **Gamification:**  Consider adding elements of gamification, like points or rewards, to encourage engagement and build motivation. \n",
      "\n",
      "Let me know if you have any specific questions about implementing these features! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task -> LLM will break it down into actionable steps and time it takes for each step -> Optional: Asking user if they want to change anything -> Start screen recording and shows the first task -> Track the task from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_breakdown(task):\n",
    "    prompt = f\"\"\"The user wants to do the following task: {task}. \n",
    "Break down the tasks into multiple, actionable steps to make the task easier to work on step by step. Give an estimate of the time needed to take to do each task\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "response = model.generate_content(task_breakdown(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Developing a Native App to Stop Procrastination\n",
      "\n",
      "This is a comprehensive project requiring expertise in mobile development, AI, and behavioral science. Hereâ€™s a breakdown of the tasks and estimated time needed:\n",
      "\n",
      "**Phase 1: Planning & Design**\n",
      "\n",
      "1. **Define Target Audience & User Needs:** Identify the target audience (students, professionals, etc.) and conduct user research to understand their procrastination patterns and motivations. (1 week)\n",
      "2. **Conceptualize App Features:** Brainstorm features based on user research and current procrastination techniques. (1 week)\n",
      "3. **Develop User Interface (UI) and User Experience (UX):** Design the user interface for a user-friendly and engaging experience. (2 weeks)\n",
      "4. **Choose Development Platform:** Decide on the target platform (Android, iOS, or both) and the development framework. (1 week)\n",
      "\n",
      "**Phase 2: Development (Android Example)**\n",
      "\n",
      "1. **Setup Development Environment:** Install necessary tools, SDKs, and libraries for Android development. (2 days)\n",
      "2. **Implement Core Functionality:** Build the foundation of the app including:\n",
      "    * **Screen Recording:** Integrate screen recording functionality using Android's MediaRecorder API. (2 weeks)\n",
      "    * **Multi-modal Model Integration:** Choose a pre-trained multi-modal model (e.g., GPT-3, CLIP) and implement integration with the app. (2 weeks)\n",
      "    * **User Interface Components:** Develop UI components for recording, task management, and analysis. (2 weeks)\n",
      "3. **Develop Analysis and Feedback Mechanisms:** \n",
      "    * **Procrastination Detection:** Utilize the multi-modal model to analyze screen recordings and detect procrastination patterns based on:\n",
      "        * **Visual cues:**  Identify distracting websites, apps, or activities. \n",
      "        * **Audio cues:**  Analyze user's audio to identify procrastination-related conversation or thoughts.\n",
      "    * **Personalized Feedback:** Provide actionable feedback and suggestions to users based on their analysis. (2 weeks)\n",
      "4. **Task Management and Motivation Features:** Implement features such as task scheduling, progress tracking, and motivational tools. (2 weeks)\n",
      "5. **Integrate with External Services:** Explore integrating with popular task management tools or productivity apps. (1 week)\n",
      "\n",
      "**Phase 3: Testing & Deployment**\n",
      "\n",
      "1. **Unit Testing:**  Test individual components and functionalities for bugs and errors. (1 week)\n",
      "2. **User Acceptance Testing (UAT):**  Get feedback from real users to identify usability issues and areas for improvement. (2 weeks)\n",
      "3. **App Store Optimization (ASO):**  Optimize app description, keywords, and visuals for app store visibility. (1 week)\n",
      "4. **App Store Submission:**  Submit the app to the Google Play Store (or App Store) for review and approval. (1 week)\n",
      "5. **Post-launch Updates and Maintenance:** Continuously monitor user feedback and implement updates to improve the app. (Ongoing)\n",
      "\n",
      "**Phase 4:  AI & Machine Learning Refinement**\n",
      "\n",
      "1. **Data Collection:**  Collect user data on usage patterns, procrastination behaviors, and feedback provided by the app. (Ongoing)\n",
      "2. **Model Training:**  Train and refine the multi-modal model with collected data to improve its accuracy and effectiveness. (Ongoing)\n",
      "3. **Personalized Recommendations:**  Develop and integrate advanced algorithms to provide more personalized feedback and recommendations. (Ongoing)\n",
      "\n",
      "**Total Estimated Time:**\n",
      "\n",
      "This project is a complex undertaking.  The estimated time for development is **approximately 12-18 months**, depending on the complexity of the features and the size of the development team.  \n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "* **Iterative Development:** Implement features incrementally and gather user feedback to refine the app throughout the development process.\n",
      "* **Data Privacy:**  Prioritize user data privacy by complying with relevant regulations and obtaining explicit consent for data usage.\n",
      "* **Ethical Considerations:** Ensure that the app promotes healthy behaviors and does not contribute to stigma or shame surrounding procrastination.\n",
      "* **Continuous Improvement:**  Invest in ongoing development, including AI model refinement, to enhance the appâ€™s effectiveness over time. \n",
      "\n",
      "Remember, this is just a general guide. The actual time required will depend on specific technical choices, team size, and the level of complexity you desire in your app. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Building a Procrastination Combat App (Mac)\n",
      "\n",
      "This breakdown assumes basic knowledge of macOS development (SwiftUI, AVFoundation, etc.). \n",
      "\n",
      "**I. Project Setup & Base Features (2-3 weeks)**\n",
      "\n",
      "1. **Set up Xcode Project:**\n",
      "   - Create a new macOS application project using SwiftUI.\n",
      "   - Add necessary dependencies:\n",
      "     - **AVFoundation:** For screen recording.\n",
      "     - **Vision Framework:** For analyzing screen content (optional).\n",
      "     - **Core ML:** For integrating your multimodal model (if you're using one).\n",
      "   - Design basic UI:\n",
      "     - Start button for recording.\n",
      "     - Time counter.\n",
      "     - Progress bar for task duration.\n",
      "     - Option to set task name and duration.\n",
      "     - UI for reviewing recordings (optional).\n",
      "   - (Estimated time: 2 days)\n",
      "\n",
      "2. **Implement Screen Recording:**\n",
      "   - Use `AVFoundation` to capture screen content (capture window, specific region, or the whole screen).\n",
      "   - Set up recording parameters:\n",
      "     - Frame rate, codec, resolution, etc.\n",
      "   - Handle recording start/stop, save recorded videos to a designated location.\n",
      "   - Display a notification to the user when recording is on/off.\n",
      "   - (Estimated time: 3 days)\n",
      "\n",
      "3. **Create Basic Task Management:**\n",
      "   - Implement a simple task management system:\n",
      "     - Add new tasks with titles and durations.\n",
      "     - Display tasks in a list.\n",
      "     - Mark tasks as completed.\n",
      "     - (Estimated time: 2 days)\n",
      "\n",
      "**II. Multimodal Model Integration (4-6 weeks)**\n",
      "\n",
      "4. **Choose a Multimodal Model:**\n",
      "   - Decide on the type of multimodal model:\n",
      "     - **Pre-trained model:** Explore available models (e.g., OpenAI's multimodal models, Google's Vision API).\n",
      "     - **Custom trained model:** Develop your own model using tools like TensorFlow or PyTorch.\n",
      "   - (Estimated time: 3 days)\n",
      "\n",
      "5. **Integrate the Model:**\n",
      "   - Set up a communication channel between your app and the model (e.g., API calls, local model integration).\n",
      "   - Prepare input for the model:\n",
      "     - Convert screen recording data (video frames, audio) into a format suitable for the model.\n",
      "     - (Estimated time: 5-7 days)\n",
      "\n",
      "6. **Interpret Model Output:**\n",
      "   - Process the model's response:\n",
      "     - Interpret the output (e.g., labels, confidence scores, predictions).\n",
      "     - Use this information to determine if the user is on task or not.\n",
      "     - (Estimated time: 5-7 days)\n",
      "\n",
      "**III. Building the User Interface & Functionality (3-4 weeks)**\n",
      "\n",
      "7. **Enhance User Interface:**\n",
      "   - Add visual feedback during recording:\n",
      "     - Display on-screen indicators to show the recording status.\n",
      "     - Provide real-time progress visualization based on model output.\n",
      "   - Design an engaging UI to motivate users:\n",
      "     - Use gamification elements (e.g., points, badges).\n",
      "     - Provide statistics and insights.\n",
      "   - (Estimated time: 5 days)\n",
      "\n",
      "8. **Implement On-Task Tracking:**\n",
      "   - Use the model's output to track if the user is focused on the task:\n",
      "     - Display warnings or alerts when the model detects procrastination.\n",
      "     - Offer suggestions to get back on track (e.g., break timers, task switching recommendations).\n",
      "   - (Estimated time: 5 days)\n",
      "\n",
      "9. **Add Additional Features (Optional):**\n",
      "   - Implement time tracking.\n",
      "   - Allow users to adjust sensitivity settings.\n",
      "   - Integrate with other productivity tools.\n",
      "   - (Estimated time: 2-3 days)\n",
      "\n",
      "**IV. Testing & Refinement (1-2 weeks)**\n",
      "\n",
      "10. **Thorough Testing:**\n",
      "   - Test the app on different macOS versions.\n",
      "   - Conduct user testing to gather feedback.\n",
      "   - Identify and fix bugs.\n",
      "   - (Estimated time: 1 week)\n",
      "\n",
      "11. **Polishing and Deployment:**\n",
      "   - Optimize performance.\n",
      "   - Finalize UI/UX.\n",
      "   - Prepare the app for submission to the Mac App Store.\n",
      "   - (Estimated time: 1 week)\n",
      "\n",
      "**Overall Timeline:**\n",
      "\n",
      "This estimate is based on a single developer working full-time. The actual time needed will depend on your experience, the complexity of the chosen model, and the scope of your app. \n",
      "\n",
      "**Note:** \n",
      "- **Multimodal Model:** The time needed for this section heavily depends on your choice of model. Using a pre-trained model might be faster, while developing a custom model would take significantly longer.\n",
      "- **User Interface:** The time for UI design and implementation will depend on your design skills and the complexity of the features you wish to add.\n",
      "\n",
      "This breakdown provides a roadmap to creating a native Mac app to combat procrastination. Remember to focus on user experience and create a tool that is helpful and enjoyable to use. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"## Building a Procrastination Combat App (Mac)\\n\\nThis breakdown assumes basic knowledge of macOS development (SwiftUI, AVFoundation, etc.). \\n\\n**I. Project Setup & Base Features (2-3 weeks)**\\n\\n1. **Set up Xcode Project:**\\n   - Create a new macOS application project using SwiftUI.\\n   - Add necessary dependencies:\\n     - **AVFoundation:** For screen recording.\\n     - **Vision Framework:** For analyzing screen content (optional).\\n     - **Core ML:** For integrating your multimodal model (if you're using one).\\n   - Design basic UI:\\n     - Start button for recording.\\n     - Time counter.\\n     - Progress bar for task duration.\\n     - Option to set task name and duration.\\n     - UI for reviewing recordings (optional).\\n   - (Estimated time: 2 days)\\n\\n2. **Implement Screen Recording:**\\n   - Use `AVFoundation` to capture screen content (capture window, specific region, or the whole screen).\\n   - Set up recording parameters:\\n     - Frame rate, codec, resolution, etc.\\n   - Handle recording start/stop, save recorded videos to a designated location.\\n   - Display a notification to the user when recording is on/off.\\n   - (Estimated time: 3 days)\\n\\n3. **Create Basic Task Management:**\\n   - Implement a simple task management system:\\n     - Add new tasks with titles and durations.\\n     - Display tasks in a list.\\n     - Mark tasks as completed.\\n     - (Estimated time: 2 days)\\n\\n**II. Multimodal Model Integration (4-6 weeks)**\\n\\n4. **Choose a Multimodal Model:**\\n   - Decide on the type of multimodal model:\\n     - **Pre-trained model:** Explore available models (e.g., OpenAI's multimodal models, Google's Vision API).\\n     - **Custom trained model:** Develop your own model using tools like TensorFlow or PyTorch.\\n   - (Estimated time: 3 days)\\n\\n5. **Integrate the Model:**\\n   - Set up a communication channel between your app and the model (e.g., API calls, local model integration).\\n   - Prepare input for the model:\\n     - Convert screen recording data (video frames, audio) into a format suitable for the model.\\n     - (Estimated time: 5-7 days)\\n\\n6. **Interpret Model Output:**\\n   - Process the model's response:\\n     - Interpret the output (e.g., labels, confidence scores, predictions).\\n     - Use this information to determine if the user is on task or not.\\n     - (Estimated time: 5-7 days)\\n\\n**III. Building the User Interface & Functionality (3-4 weeks)**\\n\\n7. **Enhance User Interface:**\\n   - Add visual feedback during recording:\\n     - Display on-screen indicators to show the recording status.\\n     - Provide real-time progress visualization based on model output.\\n   - Design an engaging UI to motivate users:\\n     - Use gamification elements (e.g., points, badges).\\n     - Provide statistics and insights.\\n   - (Estimated time: 5 days)\\n\\n8. **Implement On-Task Tracking:**\\n   - Use the model's output to track if the user is focused on the task:\\n     - Display warnings or alerts when the model detects procrastination.\\n     - Offer suggestions to get back on track (e.g., break timers, task switching recommendations).\\n   - (Estimated time: 5 days)\\n\\n9. **Add Additional Features (Optional):**\\n   - Implement time tracking.\\n   - Allow users to adjust sensitivity settings.\\n   - Integrate with other productivity tools.\\n   - (Estimated time: 2-3 days)\\n\\n**IV. Testing & Refinement (1-2 weeks)**\\n\\n10. **Thorough Testing:**\\n   - Test the app on different macOS versions.\\n   - Conduct user testing to gather feedback.\\n   - Identify and fix bugs.\\n   - (Estimated time: 1 week)\\n\\n11. **Polishing and Deployment:**\\n   - Optimize performance.\\n   - Finalize UI/UX.\\n   - Prepare the app for submission to the Mac App Store.\\n   - (Estimated time: 1 week)\\n\\n**Overall Timeline:**\\n\\nThis estimate is based on a single developer working full-time. The actual time needed will depend on your experience, the complexity of the chosen model, and the scope of your app. \\n\\n**Note:** \\n- **Multimodal Model:** The time needed for this section heavily depends on your choice of model. Using a pre-trained model might be faster, while developing a custom model would take significantly longer.\\n- **User Interface:** The time for UI design and implementation will depend on your design skills and the complexity of the features you wish to add.\\n\\nThis breakdown provides a roadmap to creating a native Mac app to combat procrastination. Remember to focus on user experience and create a tool that is helpful and enjoyable to use. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
